骆驼(Luotuo): Chinese-alpaca-lora <br/>
https://github.com/LC1332/Chinese-alpaca-lora <br/>
<br/>
Stanford Alpaca: An Instruction-following LLaMA Model <br/>
https://github.com/tatsu-lab/stanford_alpaca <br/>
<br/>
LLaMA <br/>
https://github.com/facebookresearch/llama <br/>
模型文件下载 <br/>
https://ipfs.io/ipfs/Qmb9y5GCkTG7ZzbBWMu2BXwMkzyCKcUjtEKPpgdZ7GEFKm/ <br/>
<br/>
有关的一些新微调方式 <br/>
https://huggingface.co/blog/stackllama <br/>
https://github.com/lm-sys/FastChat/#fine-tuning <br/>
<br/>
清华 <br/>
https://github.com/THUDM/ChatGLM-6B <br/>
https://models.aminer.cn/glm-130b/ <br/>
https://github.com/THUDM/GLM-130B <br/>
glm-10b <br/>
https://github.com/THUDM/GLM#generation <br/>
ChatGLM创意 <br/>
https://github.com/ypwhs/CreativeChatGLM <br/>
https://lslfd0slxc.feishu.cn/docx/JAgsd5WVgoZ1bKxYzq0c8gh5neh <br/>
<br/>
清华模型微调 <br/>
https://github.com/mymusise/ChatGLM-Tuning <br/>
https://github.com/liucongg/ChatGLM-Finetuning <br/>
<br/>
<br/>
BELLE(基于BLOOM和LLAMA针对中文做了优化) <br/>
https://github.com/LianjiaTech/BELLE <br/>
https://huggingface.co/BelleGroup/BELLE-7B-2M <br/>
<br/>
<br/>
中文LLaMA模型和经过指令精调的Alpaca大模型 <br/>
https://github.com/ymcui/Chinese-LLaMA-Alpaca <br/>
<br/>
Chinese-alpaca-lora <br/>
https://github.com/fecet/alpaca-lora-Chinese <br/>
https://github.com/tloen/alpaca-lora <br/>
<br/>
开源指令数据集 <br/>
https://github.com/yanqiangmiffy/InstructGLM <br/>
<br/>
<br/>
https://huggingface.co/IDEA-CCNL/Randeng-TransformerXL-5B-Deduction-Chinese <br/>
https://huggingface.co/IDEA-CCNL/Randeng-TransformerXL-5B-Abduction-Chinese <br/>
<br/>
提示的写法 <br/>
https://jerryzou.com/posts/how-to-write-a-prompt-for-chatgpt/ <br/>
<br/>
HuggingGPT <br/>
https://github.com/microsoft/JARVIS <br/>
<br/>
GPT4ALL <br/>
https://github.com/nomic-ai/gpt4all <br/>
<br/>
AutoGPT <br/>
https://github.com/Significant-Gravitas/Auto-GPT <br/>
<br/>
MOSS <br/>
https://github.com/OpenLMLab/MOSS <br/>
<br/>
RLHF <br/>
https://github.com/sunzeyeah/RLHF <br/>
